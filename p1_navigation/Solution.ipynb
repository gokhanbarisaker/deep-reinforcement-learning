{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6302254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b40b8b",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\t\nUnity brain name: BananaBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 37\n        Number of stacked Vector Observation: 1\n        Vector Action space type: discrete\n        Vector Action space size (per agent): 4\n        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/Applications/Banana.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda90486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e12e2a",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of agents: 1\nNumber of actions: 4\nStates look like: [1.         0.         0.         0.         0.84408134 0.\n 0.         1.         0.         0.0748472  0.         1.\n 0.         0.         0.25755    1.         0.         0.\n 0.         0.74177343 0.         1.         0.         0.\n 0.25854847 0.         0.         1.         0.         0.09355672\n 0.         1.         0.         0.         0.31969345 0.\n 0.        ]\nStates have length: 37\n[[1.         0.         0.         0.         0.84408134 0.\n  0.         1.         0.         0.0748472  0.         1.\n  0.         0.         0.25755    1.         0.         0.\n  0.         0.74177343 0.         1.         0.         0.\n  0.25854847 0.         0.         1.         0.         0.09355672\n  0.         1.         0.         0.         0.31969345 0.\n  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "\n",
    "print(env_info.vector_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8216be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, brain_name, n_episodes = 2000, max_t = 1000, eps_start = 1.0, eps_end = 0.01, eps_decay = 0.995):\n",
    "\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        score = 0                                          # initialize the score\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action)\n",
    "            \n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            mean_scores = np.mean(scores_window)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(i_episode % 100)\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"test\")\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_scores))\n",
    "        if mean_scores>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.q_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "                \n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps * eps_decay)\n",
    "        \n",
    "        print(\"Episode {}, Average score: {:.4f}\".format(i_episode, score))\n",
    "    print(\"Completed training\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0fe5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play(env, agent, brain_name, n_episodes = 2000, max_t = 1000, eps_start = 1.0, eps_end = 0.01, eps_decay = 0.995):\n",
    "#     env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "#     state = env_info.vector_observations[0]            # get the current state\n",
    "#     score = 0                                          # initialize the score\n",
    "\n",
    "#     for i_episode in range(1, n_episodes):\n",
    "#         # TODO: Grap an action from the agent\n",
    "\n",
    "\n",
    "#         for t in range(max_t):\n",
    "#             action = agent.act(state)\n",
    "#             env_info = env.step(action)\n",
    "            \n",
    "#             env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#             next_state = env_info.vector_observations[0]   # get the next state\n",
    "#             reward = env_info.rewards[0]                   # get the reward\n",
    "#             done = env_info.local_done[0]                  # see if episode has finished\n",
    "            \n",
    "#             agent.step(state, action, reward, next_state, done)\n",
    "#             state = next_state\n",
    "#             score += reward\n",
    "\n",
    "#             if done:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0bb4589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/drlnd/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/Caskroom/miniconda/base/envs/drlnd/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "0\n",
      "test\n",
      "Episode 0\tAverage Score: nan\n",
      "Episode 0, Average score: 0.0000\n",
      "1\n",
      "Episode 1, Average score: 3.0000\n",
      "2\n",
      "Episode 2, Average score: 0.0000\n",
      "3\n",
      "Episode 3, Average score: 0.0000\n",
      "4\n",
      "Episode 4, Average score: 0.0000\n",
      "5\n",
      "Episode 5, Average score: 0.0000\n",
      "6\n",
      "Episode 6, Average score: -1.0000\n",
      "7\n",
      "Episode 7, Average score: 0.0000\n",
      "8\n",
      "Episode 8, Average score: 0.0000\n",
      "9\n",
      "Episode 9, Average score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "agent = Agent(state_size, action_size, device)\n",
    "\n",
    "train(env, agent, brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2377ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, agent):\n",
    "    \"\"\" Play \"\"\"\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]  \n",
    "    score = 0\n",
    "    done = False\n",
    "    \n",
    "    # while not done:\n",
    "        \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d32e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}